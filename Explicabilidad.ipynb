{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELDA: Importaciones y carga de datos + modelo para explicabilidad ===\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from your_model_file import FineTuneResNet50  # ajusta al nombre de tu módulo\n",
    "\n",
    "# 1) Cargar checkpoint\n",
    "ckpt_path = \"checkpoints/resnet50_animals_exp.pth\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# 2) Reconstruir transform\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize(checkpoint[\"transform\"][\"resize\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=checkpoint[\"transform\"][\"normalize_mean\"],\n",
    "                         std= checkpoint[\"transform\"][\"normalize_std\"])\n",
    "])\n",
    "\n",
    "# 3) Cargar dataset y split\n",
    "images_dir = r\"C:\\Users\\juanj\\Desktop\\Reconocimineto-AnimalesDomesticos-CNN-Explicabilidad\\data\\images\"\n",
    "full_ds = datasets.ImageFolder(root=images_dir, transform=tf)\n",
    "n = len(full_ds)\n",
    "n_train = int(checkpoint[\"train_val_ratio\"] * n)\n",
    "n_val   = n - n_train\n",
    "_, val_ds = random_split(full_ds, [n_train, n_val],\n",
    "                         generator=torch.Generator().manual_seed(checkpoint[\"split_seed\"]))\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 4) Reconstruir y cargar modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FineTuneResNet50(num_classes=checkpoint[\"num_classes\"])\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 5) Preparar Captum\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# 6) Función de visualización\n",
    "def show_attr(image, attr, title=\"Attribution\"):\n",
    "    attr = attr.sum(dim=0).cpu().detach().numpy()\n",
    "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
    "    plt.imshow(image.permute(1,2,0).cpu(), alpha=0.8)\n",
    "    plt.imshow(attr, cmap='hot', alpha=0.4)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ¡Listo! Ahora puedes usar el bloque de IG:\n",
    "# images, labels = next(iter(val_loader))\n",
    "# img, lbl = images[0:1].to(device), labels[0].item()\n",
    "# attr_ig, _ = ig.attribute(img, target=lbl, return_convergence_delta=True)\n",
    "# show_attr(img[0], attr_ig[0], title=f\"IG para clase {lbl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(val_loader))\n",
    "img, lbl = images[0:1].to(device), labels[0].item()\n",
    "attr_ig, _ = ig.attribute(img, target=lbl, return_convergence_delta=True)\n",
    "show_attr(img[0], attr_ig[0], title=f\"IG para clase {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "def show_attr(image, attr, title=\"Attribution\"):\n",
    "    attr = attr.sum(dim=0).cpu().detach().numpy()\n",
    "    # normalizamos para visualizar\n",
    "    attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8)\n",
    "    plt.imshow(image.permute(1,2,0).cpu(), alpha=0.8)\n",
    "    plt.imshow(attr, cmap='hot', alpha=0.4)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo con un batch de validación\n",
    "model.eval()\n",
    "images, labels = next(iter(val_loader))\n",
    "img = images[0:1].to(device)\n",
    "lbl = labels[0].item()\n",
    "\n",
    "# Calculamos atribuciones\n",
    "attr_ig, _ = ig.attribute(img, target=lbl, return_convergence_delta=True)\n",
    "\n",
    "# Visualizamos\n",
    "show_attr(img[0], attr_ig[0], title=f\"IG para clase {lbl}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
